

\textbf{Beyond binary dependencies in the structure of the world’s languages}

\textbf{Damián E. Blasi}\textbf{\textsuperscript{1,2}}\textbf{ and Seán G. Roberts}\textbf{\textsuperscript{3}}

\textbf{\textit{1 University of Zürich}}

\textbf{\textit{2 Max Planck Institute for the Science of Human History}}

\textbf{\textit{3 Max Planck Institute for Psycholinguistics}}

The study of the regularities in the structures present across languages has always been a quest in close contact with the analysis of data.\textbf{} Traditionally, causal dependencies between pairs of typological variables (like word order patterns or the composition of segment inventories) have been argued for on the basis of language counts, namely how many languages out of a sample exhibit certain pattern in contrast to other. Regularities of this kind have been used in virtually all theoretical camps, and researchers made them part of their discussion on functional pressures on language, cognitive schemes and the architecture of a putative common computational core underlying language, among others. This popularity resides, without doubt, in the strength and the simplicity of the idea: if a set of languages with no recent genealogical history nor traces of areal contact tend to share the same pair of properties again and again, then there seems to be under scrutiny something about the properties of \textit{probable} languages in general. 

While venerable and potentially useful, this procedure is complicated by many factors.  First, the nature of a proposed dependency can affect how the pattern of observations translates into support for the dependency.  In the first section, we show how different notions of causality and causal strength are appropriate for different types of dependencies involving two variables.  Secondly, these dependencies can be distorted not only by historical relations between languages (as usually acknowledged in the literature) but also due to complex causal dependencies involving multiple variables.  Addressing these concerns requires appropriate formalisms and statistical techniques. These exist and are widely used for addressing the problem of historical relations (which we cover in the second section), but methods for dealing with relationships between more than two variables are underdeveloped in linguistics. In the final section, we discuss some new approaches to detecting causal dependencies between more than two variables.

\textbf{Probability and causation}

There exist several possible formalizations of the concept of causality inspired in concepts from mathematics, logic, computation and philosophy (see Fitelson \& \citealt{Hitchcock2011}). For the kind of kind of regularities and laws governing the language sciences - contingent, not deterministic, but with varying degree of intensity or extension - causation appears more naturally described in terms of probabilities\footnote{ Note that classic deterministic relations can be accommodated in this framework as well. A deterministic law is can be described as having a probability of 1.}. 

For the sake of simplicity, we will be dealing in these examples with a hypothesized cause (C) and an effect (E). These will be expressed in terms of total probabilities of the cause or the effect to occur (P(C) and P(E) respectively) and the related conditional probabilities (such as the chance of the effect occurring given the cause is present P(E{\textbar}C), or the probability of the effect occurring given that the cause is absent P(E{\textbar}{\textasciitilde}C)).  In this context, we can think about causation as \textit{probability raising}: the probability of the effect taking place is larger when the cause is present than when the cause is absent, P(E{\textbar}C){\textgreater}P(E{\textbar}{\textasciitilde}C)\footnote{ In our case (where either C occurs or does not) this implies that  P(E{\textbar}C){\textgreater}=P(E). This guarantees that the causality measures we discuss later take values between 0 (no causal link) to 1 (perfect causal link) with intermediate values reflecting degrees of influence.}. 

It is critical to remark that these probabilities and the measures of strength are used as a way of \textit{thinking} about causal relations instead of definitions suitable for statistical analysis. Identifying probabilities with type frequencies and determining causal dependencies by attesting patterns in language counts can be problematic, and as such the structure of the models we use to think about the data and the data themselves (and their statistical properties) should be always clearly distinguished.

Typically, probabilities are equated to frequencies of occurrence when the statistical assessment takes place. P(E) is approximated to the proportion of times the cause is observed to occur compared to not occurring, and P(E{\textbar}{\textasciitilde}C) to the fraction of times the effect is observed when the cause is absent. For instance, given the contingency table

\begin{tabularx}{\textwidth}{XXX} & C & {\textasciitilde}C\\
\lsptoprule
 E & 10 & 5\\
 {\textasciitilde}E & 5 & 25\\
\lspbottomrule
\end{tabularx}
we could readily estimate P(C)=15/45=1/3 and P(E{\textbar}{\textasciitilde}C)=5/25=1/5. This is the usual practice in the field, but it hides a number of assumptions about what is it tested and the nature of the sampling process.

First of all, the strategy of counting languages has been used sometimes to say something about \textit{probable} languages in general and not about the particular relations that hold in the necessarily contingent set of surveyed languages. This circumstance is as fundamental as it is uncontroversial and pervades the scientific practice, and in particular the language sciences - we infer general properties of cognition from limited sample of experimental participants and we determine the usage properties of words from samples of text that are diminishingly small in comparison to what is regularly produced by speakers. 

In consequence, we assume that the frequency measured in a given set of typological data matches, in some way, the likelihood of picking at random any \textit{likely} human language and find that it has such property. This becomes explicit in the linguistic typology literature: in the absence of mechanisms or constraints shaping the structure of the grammar, we “would expect each type to have roughly an equal number of representatives” \citep{Comrie1989}. The issue stems from the fact that what “roughly” means here is left unspecified and to a large extent at the discretion of the researcher. In fact, any reasonable sampling model will generate observable differences in the proportions even when no effect is present \citep{Cysouw2003}. Specific distributions of typological variables have been motivated observationally \citep{Nicholls1992}, based on concrete models inspired on principles of language change \citep{Cysouw2010} or borrowed directly from the toolkit of machine learning, the Dirichlet process being a particularly popular choice that is plastic enough as to reflect our lack of certainty (\citealt{Daume2009}, Li et al 2009, Piantadosi \& \citealt{Gibson2014}).  

Assuming for a moment now that we do have access to the \textit{true} probabilities of the causes and effects and their relation (perhaps via a careful consideration of the observed frequencies), let us consider now that the two simplest cases of causal relations between C and E (illustrated in \figref{fig:1}). Greenberg’s seminal work on implicational typological universals already presented a binary classification of dependencies into which we will tap due to its popularity \citep{Greenberg1966}\footnote{ The distinction between “absolute” and “statistical” in Greenberg’s work does not play any role here, as the first is a limit case of the second.}. 

Some of Greenberg’s universals are \textit{bidirectional implications}, such as the order of adposition and noun implying the order of genitive and noun, \textit{and vice versa}.  Bidirectional implications contrast with \textit{unidirectional implications}, which allow the possibility of the effect being present without the cause, but the cause makes the effect more probable. For instance, Greenberg suggested that languages with VSO canonical order tend to be prepositional, though this does not claim that all prepositional languages will be VSO: prepositions occur with virtually all other word order combinations, prominently SVO.

While these ideas are intuitive, the formalization of causal strength by means of probabilities sheds light on the kinds of evidence that are needed in order to put forward any claim about causal influence.  For the sake of convenience, causal measures are often defined in such a way that 1 stands for the strongest causal relation and 0 for the absence of any evidence of a causal effect, with intermediate values reflecting strengths between these extremes. To start with, Eells’ (1991) view of causal strength captures adequately the causal strength underlying a bidirectional implication, which is defined as:

CSe = \textit{P}(\textit{E{\textbar}C}) \textit{– P}(\textit{E{\textbar}{\textasciitilde}C})

That is, the change in the probability of the effect when the cause is present and when the cause is absent. The largest difference (CSe=1) will be achieved when the cause deterministically triggers the effect (P(E{\textbar}C)=1) and where the absence of the cause also implies the absence of the effect (P(E{\textbar}{\textasciitilde}C)=0) - as represented in \figref{fig:1}. On the other hand, when the cause does not change the probability of the effect to occur (P(E{\textbar}C)=P(E{\textbar}{\textasciitilde}C)), Eells' measure of causal strength is minimised (CSe=0). Notice that the strength of the assertion of a bidirectional implicational universal does not rely on the relative frequencies of each type, i.e. P(C) and P(E) and their complements.

On the other hand, unidirectional implications do not make any predictions with respect to the case in which the cause is absent. P(E{\textbar}{\textasciitilde}C) could be close to either 1 or 0 without affecting our confidence on the efficacy of the cause - e.g. that smoking leads convincingly to cancer is independent of the fact that it might arise due to other factors as well. However, rather than using the plain conditional probability as a measure of the causal strength of a unidirectional implication (P(E{\textbar}C)) the probability P(E{\textbar}{\textasciitilde}C) plays the role of a baseline to which to compare against. Thus, a good normalized measure of causal strength for unidirectional implications would be one that (1) becomes 0 when the cause does not make the effect more or less probable than its absence and (2) is 1 only when the cause yields the effect determinstically (P(E{\textbar}C)=1). This leads to no other than Cheng’s (1997) notion of causal strength:

CSc = [P(E{\textbar}C) - P(E{\textbar}{\textasciitilde}C)] / P({\textasciitilde}E{\textbar}{\textasciitilde}C)

That is, the causal power increases as we observe the effect with the cause and decreases as we observe the effect with the cause, but only to the extent that we also observe no effect without the cause.  

  
%%please move the includegraphics inside the {figure} environment
%%\includegraphics[width=\textwidth]{blasi-img1.png}
 

\textbf{\figref{fig:1}: Contingency tables that maximise different measures of causal strength when language type frequencies are equated to type probabilities.  On the left are two tables which maximize unidirectional implications and on the right are two tables which maximize bidirectional implication.  More intense colour stands for more cases attested with those properties; cells in white represent no counts. The formulas for different notions of causal strength appear at the bottom}

In contrast to the idea that causality constitutes a monolithic phenomenon, there are many other approaches to the notion of causal strength (see  Fiddleston \& Hitchcock, 2011), each one being suitable for the study of different dependencies.   The notion of causal measure will also impact the strategy of inference of the involved probabilities. For example, a unidirectional implication could be assessed by collecting data only on languages which are known to exhibit the cause, while a bidirectional implication requires knowing about languages both with and without the cause. 

\textbf{Moving towards statistical support} 

The formalisms above rely on knowing the real probabilities of each cell in the contingency table.  The question of practical interest, then, is then how to make a statistically valid case for a dependency based on language counts.  These counts might differ considerably from the true probabilities since simple co-occurrence in a sample of data does not guarantee \textit{dependency}. The most well-known sources of inflated co-occurrences without substantial causal links are shared history or contact. For instance, in the Meso-American linguistic area, languages frequently display a vigesimal numeral system and they lack switch-reference, traits the distinguish them from neighboring languages \citep{CampbellEtAl1986}. A contingency table displaying the number of languages in that region would give the impression that both variables are associated, which will be simply reflecting the fact that those traits have been transmitted together all the way down the genealogical tree or horizontally from other language(s). This confound - known as Galton’s problem - applies to any study trying to detect causal connections between traits in languages.  Roberts \& \citet{Winters2013} demonstrate how pervasive this problem can be by finding co-occurrences between traits with no causal dependencies between them.  

These problems can be overcome if the history of contact between languages is taken into account.  For example, bidirectional implications can be easily captured by the many regression methods available. \citet{JaegerEtAl2011} recommend a mixed effects model framework so to be able to account for areal and genealogical dependencies as random effects for that purpose. Another alternative is to use explicit phylogenetic information and map branch lengths to covariance (so languages that diverged more recently in time are expected to have more similar feature values) \citep{Verkerk2014}. The Family Bias method \citep{Bickel2013} continues the tradition of comparing larger linguistic groupings in a regular regression setting (without any special specification of the covariance between languages) but instead infers the biases of the groupings by assessing the evidence in favour or against one particular typological variant (or set of variants).

The literature on the statistical assessment of unidirectional implications is much less restricted, however. Researchers have devised ways of resolving this issue within the frequentist \citep{EverettEtAl2015} and Bayesian traditions (Daumé and \citealt{Campbell2007}).

Another way that co-occurrence probabilities can be distorted, and one that is rarely addressed, involves more complicated causal dependencies.  The statistical methods mentioned above become limited when more than two variables are taken into account at a time and indeed, perhaps as an implicit acknowledgment of this difficulty, most typological generalizations are limited to pairs of variables rather than more complex constellations. 

Let us see more precisely how complex dependencies might yield spurious dependencies by considering the simplest possible case beyond the two-variable case, which is naturally when there are three variables causally linked in some way. If we regard causal relations graphically as arrows going from the causes to the effects, then this setting will correspond to any of four different possible arrangements depicted in \figref{fig:2}.

  
%%please move the includegraphics inside the {figure} environment
%%\includegraphics[width=\textwidth]{blasi-img2.png}
 

\textbf{\figref{fig:2}:  Four possible (non-trivial) ways in which variables X, Y and Z could be causally linked.  Arrows represent the flow of causality, so that an arrow pointing from X to Z indicates that changes to X cause changes in Z.}

The first two cases (a and b in \figref{fig:2}) correspond to Z simply serving as a communicator of the effect of X on Y or vice versa. For instance, it has been suggested that population size and morphological complexity are causally connected via the number of L2 speakers (Lupyan and Dale, 2010): the larger the population (X) , the more likely it is that the language comes into contact with others, increasing the number of L2 speakers (Z) which act upon the language by simplifying the system of its morphological structures (Y).

The third possibility is that Z is causing both X and Y (d in \figref{fig:2}), so the observed causal link between the two is an artefact of Z being a common cause. As an example, many languages of the world have noun classes (X) have also applicative voice (Y) (Aronoff and \citealt{Fudeman2011}). The common cause behind the joint occurrence of these features is that many of these languages come from the Atlantic-Congo family (Z), one of the largest linguistic families.

Finally, it could be that both X and Y contribute jointly to cause Z (c in \figref{fig:2}). Languages with isolating morphology (X) will naturally have shorter words in average (Z), and the same is true for languages with tones (Y).

The qualitative Greenbergian implications presented before had a transparent formal counterpart and they can be evaluated statistically with well established methods. However, the discussion and evaluation of dependencies involving three or more variables become increasingly unsuitable without a proper formalization. The probabilistic framework discussed at the beginning finds a justification at this point. In addition to it, we need to briefly review some definitions and concepts from graph theory (see Pearl, 2009).

A graph consists of a set of nodes and a set of edges. Directed edges bind two nodes in an asymmetrical fashion- so if A and B are nodes, either A→B or A←B. A sequence of nodes from A to B where each adjacent pair is bound by a directed edge going from the first to the second member is referred to as a \textit{path} between A and B. A path that starts and finishes in the same node is referred to as a \textit{cycle}.  A directed graph is one in which all edges are directed, and a directed graph with no cycles is called a directed acyclic graph (DAG).

The set of nodes that can be reached through a path from A are A’s \textit{descendants},\textit{} and the nodes that are directly connected to A such that their common edge points to A (like B→A) are the \textit{parents} of A.  In DAGs there are no paths which go from a descendant back to one of its parents. 

This graphical framework allows a straightforward visualization of causal connections between variables.  Variables are represented as nodes and causal relations (of any kind discussed in the binary case) are represented as directed edges, so A→B will be read as “A causes B”. The assumption linking this graph representation to the ideas of probabilistic causation discussed before is that of the Markov Causal Condition. If two variables are dependent and one is not a descendent of the other then their dependency can be explained away by appealing to a common ancestor of the pair.  Put another way, a variable is only affected by its immediately connected (ancestor) causes.

Embracing this representation of the relations in the data opens up new statistical possibilities. One that partially relies on regression is to use structural equation models (Duncan, 2014). Structural equation modelling is a cover term for a number of techniques that allows the testing of more or less well-specified functional dependencies between variables as embedded in DAGs. To take a very basic example (based on a specific case of structural equation modelling called \textit{path analysis}), suppose that we want to decide between situations (a) and (b) of \figref{fig:2}. Assuming that we are in possession of good guesses about what could be the functional dependencies, we then could contrast the model fit (how well the model predicts the observed data) between (a) and (b). The possibilities provided by structural equation modelling include the inclusion of hidden variables and non-parametric functional dependencies.

In cases where the uncertainty about the correct model is high, model comparison might not be the best ally. In those cases, it is possible to appeal to the predictions that come “for free” by assuming the Markov Causal Condition along with the DAG. The idea is that the Markov Causal Condition entails a series of conditional dependency statements involving the variables, and that given appropriate conditions it is possible to estimate the most likely underlying causal graph from observational data.  There are multiple methods for doing this (Shalizi, 2013), a popular efficient and computationally inexpensive method being the PC algorithm (Spirtes \& Scheines, 2000; Kalisch et al., 2012). These techniques are only starting to be explored by researchers in the language sciences (Blasi, Roberts and Maathuis in prep).

\textbf{Conclusion}

The inference of causal dependencies based on surveys of languages has a long history in the field. This methodology faces several complications, like the difficulty of estimating probabilities from counts of languages or the lack of consideration of higher-order dependencies between multiple variables. Methods and formalisms based on probability can address these problems, and help linguists to better test and think about the nature of dependencies in language.

\textbf{Acknowledgements}

We would like to thank the participants of the Dependencies workshop, and in particular Jennifer Culbertson for helpful feedback.  SR is supported by an ERC Advanced Grant No. 269484 INTERACT to Stephen Levinson. We thank the Max Planck Society for additional support.
\begin{verbatim}%%move bib entries to  localbibliography.bib
@misc{Agresti2014,
	author = {Agresti, A},
	title = {\textit{Categorical data analysis}. John Wiley \& Sons.},
	year = {2014}
}

@misc{Aronoff2011,
	author = {Aronoff, M.,  and  Fudeman, K},
	number = {8},
	series = {Vol.},
	title = {\textit{What is morphology}},
	year = {2011}
}

@article{Bakker2008,
	author = {Bakker, D},
	journal = {STUF-Language Typology and Universals Sprachtypologie und Universalienforschung},
	number = {3},
	pages = {186-198},
	title = {Linfer: Inferring implications from the wals database},
	volume = {61},
	year = {2008}
}

Bickel, B. (2013). Distributional biases in language families. \textit{Language typology and historical contingency}, 415-444.

Blasi, D., Roberts, S. and Maathuis, M. (in prep) Inferring causality in lexical properties from observational data.

Campbell, L., Kaufman, T., \& Smith-Stark, T. C. (1986). Meso-America as a linguistic area. \textit{Language}, 530-570.

Cysouw, M. (2010). On the probability distribution of typological frequencies. In The mathematics of language (pp. 29-35). Springer Berlin Heidelberg.

@book{Cheng1997,
	address = {\textit{Psychological Review} 104},
	author = {Cheng, P.},
	publisher = {367 – 405},
	title = {'From Covariation to Causation: A Causal Power Theory'},
	year = {1997}
}

@book{Corbett1991,
	address = {\textit{Cambridge},
	author = {Corbett, G.},
	publisher = {Cambridge}},
	title = {Gender. Cambridge textbooks in linguistics},
	year = {1991}
}

Daumé III, H. (2009). Non-parametric Bayesian areal linguistics. In Proceedings of human language technologies: \citealt{The2009} annual conference of the north american chapter of the association for computational linguistics (pp. 593-601). Association for Computational Linguistics.

@misc{Duncan2014,
	author = {Duncan, O. D},
	title = {\textit{Introduction to structural equation models}. Elsevier.},
	year = {2014}
}

@book{Eells1991,
	address = {Cambridge,},
	author = {Eells, E.},
	publisher = {Cambridge University Press},
	title = {Probabilistic Causality},
	year = {1991}
}

Everett, C., Blasi, D. E., \& Roberts, S. G. (2015). Climate, vocal folds, and tonal languages: Connecting the physiological and geographic dots. \textit{Proceedings of the National Academy of Sciences}, \textit{112}(5), 1322-1327.

Fitelson, B \& Hitchcock, C. (2011) Probabilistic measures of causal strength.  In P. McKay Illari, F. Russo \& J. Williamson (Eds.) Causality in the Sciences.  Oxford University Press.

@book{Greenberg1966,
	address = {Cambridge, MA},
	author = {Greenberg JH.},
	publisher = {MIT Press},
	title = {\textit{Universals of Language}},
	year = {1966}
}

Hyman, L. M. (1977). On the nature of linguistic stress. In Hyman (ed.) \textit{Studies in stress and accent}, 4. pp. 37-82.

@article{Hyman2008,
	author = {Hyman, L. M},
	journal = {The Linguistic Review},
	pages = {81–135},
	title = {Universals in phonology},
	volume = {25},
	year = {2008}
}

Jaeger, T. F., Graff, P., Croft, W., \& Pontillo, D. (2011). Mixed effect models for genetic and areal dependencies in linguistic typology. \textit{Linguistic Typology}, \textit{15}(2), 281-320.

@book{Jaynes2003,
	address = {\textit{Probability theory},
	author = {Jaynes, E. T.},
	publisher = {the logic of science}. Cambridge university press},
	year = {2003}
}

@book{Kalisch2012,
	address = {Journal of Statistical Software, 47(11)},
	author = {Kalisch, M., Mächler, M., Colombo, D., Maathuis, M. H., and Bühlmann, P.},
	publisher = {1– 26},
	title = {Causal inference using graphical models with the R package \textit{pcalg}},
	year = {2012}
}

Lewis, D. (1986). Postscripts to ‘Causation’, in Philosophical Papers, Volume II.Oxford University Press, pp. 173–213.

Lin, C. C., Wang, Y. C., \& Tsai, R. T. H. (2009). Modeling the Relationship among Linguistic Typological Features with Hierarchical Dirichlet Process. In PACLIC (pp. 741-747).

@book{Lupyan2010,
	address = {\textit{PLoS ONE}, 5(1)},
	author = {Lupyan, G. and Dale, R.},
	publisher = {e8559},
	title = {Language structure is partly determined by social structure},
	year = {2010}
}

@article{Maslova2003,
	author = {Maslova, E},
	journal = {Linguistic typology},
	number = {1},
	pages = {101-108},
	title = {A case for implicational universals},
	volume = {7},
	year = {2003}
}

@article{Moravcsik2007,
	author = {Moravcsik, E. A},
	journal = {Linguistic Typology},
	number = {1},
	pages = {27-41},
	title = {What is universal about typology?},
	volume = {11},
	year = {2007}
}

@misc{Nichols1992,
	author = {Nichols, J},
	title = {Linguistic diversity in space and time. University of Chicago Press.},
	year = {1992}
}

@misc{Spirtes2000,
	author = {Spirtes, P., Glymour, C. and Scheines, R},
	title = {\textit{Causation, Prediction, and Search}, 2nd edition. The MIT Press.},
	year = {2000}
}

@misc{Pearl2009,
	author = {Pearl, J},
	title = {\textit{Causality}. Cambridge University Press.},
	year = {2009}
}

@article{Piantadosi2014,
	author = {Piantadosi, S. T.,  and  Gibson, E},
	journal = {Cognitive science},
	number = {4},
	pages = {736-756},
	title = {Quantitative standards for absolute linguistic universals},
	volume = {38},
	year = {2014}
}

@book{Roberts2013,
	address = {\textit{PLoS ONE}, 8(8)},
	author = {Roberts, S. G. and Winters, J.},
	publisher = {e70902},
	title = {Linguistic diversity and traffic accidents: Lessons from statistical studies of cultural traits},
	year = {2013}
}

@book{Roberts2015,
	address = {Future Tense and Economic Decisions},
	author = {Roberts, S. G., Winters, J.,  and  Chen, K.},
	publisher = {Controlling for Cultural Evolution. \textit{PloS one}, \textit{10}(7), e0132145},
	year = {2015}
}

@misc{Shalizi2013,
	author = {Shalizi, C. R},
	title = {Advanced data analysis from an elementary point of view.Preprint of book found at },
	url = {http://www. stat.cmu.edu/${\sim}$cshalizi/ADAfaEPoV.},
	year = {2013}
}

@book{Suppes1970,
	address = {Amsterdam},
	author = {Suppes, P.},
	publisher = {North-Holland Publishing Company},
	title = {\textit{A Probabilistic Theory of Causality}},
	year = {1970}
}

@article{Verkerk2014,
	author = {Verkerk, A},
	journal = {Folia Linguistica Historica},
	pages = {307-358},
	title = {The correlation between motion event encoding and path verb lexicon size in the Indo-European language family},
	volume = {35},
	year = {2014}
}

\end{verbatim}